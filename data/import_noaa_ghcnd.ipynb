{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import [GHCNd](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily) data from its [NOAA Archieve](https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/archive/)   \n",
    "<!-- Station data - US from 1950 to 2023 -->\n",
    "\n",
    "* [Country code](https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-countries.txt)\n",
    "* Station metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pySmartDL import SmartDL\n",
    "from os.path import basename\n",
    "import tarfile\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# import multiprocessing\n",
    "# from io import BytesIO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Get meta-data\n",
    "def check_data_exists(name_db: str, name_collection: str):\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    # db contains collections\n",
    "    db = client[name_db]\n",
    "    # mcd will save in each year\n",
    "    collection = db[name_collection]\n",
    "\n",
    "    # Check if collection contains data with \"year\" variable\n",
    "    if collection.count_documents( {} ) > 0:\n",
    "        print(f\"DB:{name_db}, The {name_collection} collection contains data.\")\n",
    "        # no need update, retrun empty collection\n",
    "        collection = {}\n",
    "        return collection\n",
    "    else:\n",
    "        print(f\"DB:{name_db}, The {name_collection} collection does not contain data.\")\n",
    "        return collection\n",
    "\n",
    "\n",
    "def get_ghcnd_meta(url_meta, fwf_colspec, colnames):\n",
    "    res_station = requests.get(url_meta)\n",
    "    df_station = pd.read_fwf(\n",
    "            StringIO(res_station.text),\n",
    "            colspecs=fwf_colspec,\n",
    "            names=colnames,\n",
    "            )\n",
    "    return df_station\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download_and_import_to_mongo(url, db_name, collection_name):\n",
    "    # Download the file using pySmartDL\n",
    "    dest = basename(url)\n",
    "    obj = SmartDL(url, dest)\n",
    "    obj.start()\n",
    "    \n",
    "    # Extract the .tar.gz file\n",
    "    extracted_folder = dest.replace('.tar.gz', '')\n",
    "    with tarfile.open(dest, 'r:gz') as tar:\n",
    "        tar.extractall(extracted_folder)\n",
    "    \n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    \n",
    "    # Import the .json file(s) to MongoDB\n",
    "    # (Assuming there's a .json file inside the extracted folder)\n",
    "    for root, dirs, files in os.walk(extracted_folder):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.json'):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                \n",
    "                # Open the .json file and read the data\n",
    "                with open(filepath, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                    # If data is a list of dictionaries, we can insert them all at once\n",
    "                    if isinstance(data, list):\n",
    "                        collection.insert_many(data)\n",
    "                    else:\n",
    "                        collection.insert_one(data)\n",
    "    \n",
    "    # Clean up (Optional: remove the downloaded and extracted files)\n",
    "    os.remove(dest)\n",
    "    os.rmdir(extracted_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get station metadata if not have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB:ghcnd, The meta_station collection contains data.\n",
      "DB:ghcnd, The meta_station_inventory collection contains data.\n"
     ]
    }
   ],
   "source": [
    "# station meta\n",
    "collection = check_data_exists('ghcnd', 'meta_station')\n",
    "if not collection == {} :\n",
    "    print('Get the station meta data')\n",
    "    url_meta_station = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt'\n",
    "    fwf_colspec_station = [(0, 12), (12, 21), (21, 31), (31, 38), (38, 69)]\n",
    "    col_names = [\"station\", \"lat\", \"lon\", \"elev\", \"name\"]\n",
    "    df = get_ghcnd_meta(url_meta_station, fwf_colspec_station, col_names)\n",
    "    # \n",
    "    records = df.to_dict('records')\n",
    "    collection.insert_many(records)\n",
    "\n",
    "\n",
    "\n",
    "# station meta - inventory\n",
    "collection = check_data_exists('ghcnd', 'meta_station_inventory')\n",
    "if not collection == {} :\n",
    "    print('Get the station meta - inventory data')\n",
    "    url_meta_station = 'https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt'\n",
    "    fwf_colspec_station = [(0, 12), (35, 41), (41, 46)]\n",
    "    col_names = [\"station\", \"start_year\", \"end_year\"]\n",
    "    df = get_ghcnd_meta(url_meta_station, fwf_colspec_station, col_names)\n",
    "    # \n",
    "    records = df.to_dict('records')\n",
    "    collection.insert_many(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ftp readme file\n",
    "# from ftplib import FTP\n",
    "\n",
    "# # FTP server details\n",
    "# ftp_server = 'ftp.ncdc.noaa.gov'\n",
    "# file_path = '/pub/data/ghcn/daily/readme.txt'\n",
    "# local_file = 'downloaded_file.txt'\n",
    "\n",
    "# # ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt\n",
    "\n",
    "# # Connect to the FTP server\n",
    "# ftp = FTP(ftp_server)\n",
    "# ftp.login()  # login with default user anonymous and empty password\n",
    "\n",
    "# # Set the file mode to binary\n",
    "# ftp.sendcmd(\"TYPE i\")\n",
    "\n",
    "# # Retrieve the file\n",
    "# with open(local_file, 'wb') as local_file_handle:\n",
    "#     ftp.retrbinary(f'RETR {file_path}', local_file_handle.write)\n",
    "\n",
    "# # Close the connection\n",
    "# ftp.quit()\n",
    "\n",
    "# print(f'The file has been downloaded as: {local_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select stations between 1982 to 2003 US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>elev</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USC00010008</td>\n",
       "      <td>31.5703</td>\n",
       "      <td>-85.2483</td>\n",
       "      <td>139.0</td>\n",
       "      <td>AL ABBEVILLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00010063</td>\n",
       "      <td>34.2108</td>\n",
       "      <td>-87.1783</td>\n",
       "      <td>239.6</td>\n",
       "      <td>AL ADDISON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00010140</td>\n",
       "      <td>32.2322</td>\n",
       "      <td>-87.4103</td>\n",
       "      <td>53.3</td>\n",
       "      <td>AL ALBERTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00010160</td>\n",
       "      <td>32.9350</td>\n",
       "      <td>-85.9556</td>\n",
       "      <td>201.2</td>\n",
       "      <td>AL ALEXANDER CITY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00010178</td>\n",
       "      <td>33.1272</td>\n",
       "      <td>-88.1550</td>\n",
       "      <td>59.4</td>\n",
       "      <td>AL ALICEVILLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9650</th>\n",
       "      <td>USW00094911</td>\n",
       "      <td>42.8786</td>\n",
       "      <td>-97.3636</td>\n",
       "      <td>357.2</td>\n",
       "      <td>SD YANKTON 2 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9651</th>\n",
       "      <td>USW00094918</td>\n",
       "      <td>41.3536</td>\n",
       "      <td>-96.0233</td>\n",
       "      <td>390.1</td>\n",
       "      <td>NE OMAHA #1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9652</th>\n",
       "      <td>USW00094931</td>\n",
       "      <td>47.3803</td>\n",
       "      <td>-92.8325</td>\n",
       "      <td>408.1</td>\n",
       "      <td>MN HIBBING CHISHOLM HIBBING AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9653</th>\n",
       "      <td>USW00094957</td>\n",
       "      <td>40.0792</td>\n",
       "      <td>-95.5892</td>\n",
       "      <td>298.7</td>\n",
       "      <td>NE FALLS CITY BRENNER FLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9654</th>\n",
       "      <td>USW00094967</td>\n",
       "      <td>46.8997</td>\n",
       "      <td>-95.0669</td>\n",
       "      <td>439.2</td>\n",
       "      <td>MN PARK RAPIDS MUNI AP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9655 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          station      lat      lon   elev                            name\n",
       "0     USC00010008  31.5703 -85.2483  139.0                    AL ABBEVILLE\n",
       "1     USC00010063  34.2108 -87.1783  239.6                      AL ADDISON\n",
       "2     USC00010140  32.2322 -87.4103   53.3                      AL ALBERTA\n",
       "3     USC00010160  32.9350 -85.9556  201.2               AL ALEXANDER CITY\n",
       "4     USC00010178  33.1272 -88.1550   59.4                   AL ALICEVILLE\n",
       "...           ...      ...      ...    ...                             ...\n",
       "9650  USW00094911  42.8786 -97.3636  357.2                  SD YANKTON 2 E\n",
       "9651  USW00094918  41.3536 -96.0233  390.1                     NE OMAHA #1\n",
       "9652  USW00094931  47.3803 -92.8325  408.1  MN HIBBING CHISHOLM HIBBING AP\n",
       "9653  USW00094957  40.0792 -95.5892  298.7       NE FALLS CITY BRENNER FLD\n",
       "9654  USW00094967  46.8997 -95.0669  439.2          MN PARK RAPIDS MUNI AP\n",
       "\n",
       "[9655 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client['ghcnd']\n",
    "\n",
    "\n",
    "\n",
    "df_stations_8203 = pd.DataFrame(\n",
    "        db['meta_station_inventory'].find(\n",
    "                    {   'start_year' : {'$lte'   : 1982},\n",
    "                        'end_year'   : {'$gte'   : 1982},\n",
    "                        'station'    : {'$regex' : '^US'}\n",
    "                            })\n",
    "    ).drop(columns='_id')\n",
    "\n",
    "# print(df_stations_8203.head())\n",
    "# station list also be used to get the file.\n",
    "stationlist = df_stations_8203['station'].unique().tolist()\n",
    "# \n",
    "\n",
    "# \n",
    "df_stations_8203_meta = pd.DataFrame(\n",
    "        db['meta_station'].find(\n",
    "                    {'station' : {'$in':stationlist}}\n",
    "        )\n",
    "    ).drop(columns='_id')\n",
    "# print(df_stations_8203_meta.head())\n",
    "\n",
    "df_stations_8203_meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the observation data.\n",
    "* Selecting stations by\n",
    "    * Year 1982 to 2003, US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ID = 11 character station identification code\n",
    "YEAR/MONTH/DAY = 8 character date in YYYYMMDD format (e.g. 19860529 = May 29, 1986)\n",
    "ELEMENT = 4 character indicator of element type \n",
    "DATA VALUE = 5 character data value for ELEMENT \n",
    "M-FLAG = 1 character Measurement Flag \n",
    "Q-FLAG = 1 character Quality Flag \n",
    "S-FLAG = 1 character Source Flag \n",
    "OBS-TIME = 4-character time of observation in hour-minute format (i.e. 0700 =7:00 am); if no ob time information '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_i_station(fname):\n",
    "    col_dtypes = {\n",
    "    'station': str,\n",
    "    'elem': str,\n",
    "    'value': np.float64,\n",
    "    'flag_m': str,\n",
    "    'flag_q': str,\n",
    "    'flag_s': str,\n",
    "    'time': np.float64\n",
    "    }\n",
    "    col_names = ['station', 'date', 'elem', 'value', 'flag_m', 'flag_q', 'flag_s', 'time']\n",
    "    # \n",
    "    df = pd.read_csv(fname,\n",
    "        compression='gzip',\n",
    "        header=None,\n",
    "        names=col_names,\n",
    "        dtype=col_dtypes,\n",
    "        parse_dates=['date'],\n",
    "    )\n",
    "    # \n",
    "    return df\n",
    "\n",
    "# select measured element only in \n",
    "elem_list = [   'PRCP', # Precipitation (tenths of mm)\n",
    "                'TMAX', # Maximum temperature (tenths of degrees C)\n",
    "                'TMIN', # Minimum temperature (tenths of degrees C)\n",
    "                'AWND', # Average daily wind speed (tenths of meters per second)\n",
    "\n",
    "                'ACMC', # Average cloudiness midnight to midnight from 30-second c eilometer data (percent)\n",
    "                'ACSC', # Average cloudiness sunrise to sunset from 30-second c eilometer data (percent)\n",
    "                'RHAV', # Average relative humidity for the day (percent)\n",
    "                'RHMN', # Minimum relative humidity for the day (percent)\n",
    "                'RHMX'] # Maximum relative humidity for the day (percent)\n",
    "\n",
    "\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['ghcnd']\n",
    "\n",
    "for year in range(1972,2013) : # getting +- 10years of climate data.\n",
    "    # check collection is empty - use temp\n",
    "    collection = db['station_data_{elem}'.format(elem='TMAX')]\n",
    "    if collection.count_documents({'date': {\n",
    "            '$gte': datetime(year, 1, 1),\n",
    "            '$lte': datetime(year, 12, 31)\n",
    "        }}) == 0 :\n",
    "\n",
    "        # \n",
    "        url = f'https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/{year}.csv.gz'\n",
    "        obj = SmartDL(url, progress_bar=False)\n",
    "        # if file already exists, pass\n",
    "        if not os.path.exists(obj.get_dest()) :\n",
    "            obj.start()\n",
    "\n",
    "        # Read the downloaded file into memory\n",
    "        df = read_i_station(obj.get_dest())\n",
    "        # subsetting stations in the station list.\n",
    "        df = df.loc[df['station'].isin(stationlist)]\n",
    "\n",
    "\n",
    "        for elem in elem_list :\n",
    "            df_elem = df.loc[df['elem'] == elem].reset_index(drop=True)\n",
    "            if df_elem.index.max() > 0 :\n",
    "                # normalize value and change 'value' to element value.\n",
    "                # e.g. TMAX divde by 10. \n",
    "                if elem in ['PRCP','TMAX','TMIN','AWND'] :\n",
    "                    df_elem['value'] = df_elem['value'] / 10 \n",
    "                else :\n",
    "                    df_elem['value'] = df_elem['value'] / 100\n",
    "                \n",
    "                # rename\n",
    "                df_elem = df_elem.rename(columns={'value':elem}).drop(columns='elem')\n",
    "                \n",
    "\n",
    "                # DB collection\n",
    "                collection = db[f'station_data_{elem}']\n",
    "                # check existance and resume.\n",
    "                # \n",
    "                records = df_elem.to_dict('records')\n",
    "                collection.insert_many(records)\n",
    "\n",
    "        # clear memory\n",
    "        df = {}\n",
    "        df_elem = {}\n",
    "        # delete temp file\n",
    "        os.remove(obj.get_dest())\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
